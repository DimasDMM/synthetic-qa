{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy filter for SQuAD-like dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "import json\n",
    "from math import log2\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "import statistics as st\n",
    "import string\n",
    "import threading\n",
    "import time\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACTS_PATH = '../artifacts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGS = {\n",
    "    'en': {\n",
    "        'region_code': 'en',\n",
    "        'lang_name': 'English',\n",
    "        'lang_code': 'en-EN',\n",
    "        'spacy_dict': 'en_core_web_sm',\n",
    "    },\n",
    "    'es': {\n",
    "        'region_code': 'es',\n",
    "        'lang_name': 'Spanish',\n",
    "        'lang_code': 'es-ES',\n",
    "        'spacy_dict': 'es_core_news_sm',\n",
    "    },\n",
    "    'ru': {\n",
    "        'region_code': 'ru',\n",
    "        'lang_name': 'Russian',\n",
    "        'lang_code': 'ru-RU',\n",
    "        'spacy_dict': 'ru_core_news_sm',\n",
    "    },\n",
    "    'vi': {\n",
    "        'region_code': 'vi',\n",
    "        'lang_name': 'Vietnamese',\n",
    "        'lang_code': 'vi-VN',\n",
    "        'spacy_dict': '',\n",
    "    },\n",
    "    'ja': {\n",
    "        'region_code': 'ja',\n",
    "        'lang_name': 'Japanese',\n",
    "        'lang_code': 'ja-JP',\n",
    "        'spacy_dict': 'ja_core_news_sm',\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_tokenizer(dictionary_name=None, region_code=None, basic_token=False):\n",
    "    if dictionary_name == '':\n",
    "        if region_code == 'en':\n",
    "            from spacy.lang.en import English as NlpTokenizer\n",
    "        elif region_code == 'es':\n",
    "            from spacy.lang.es import Spanish as NlpTokenizer\n",
    "        elif region_code == 'ru':\n",
    "            from spacy.lang.ru import Russian as NlpTokenizer\n",
    "        elif region_code == 'vi':\n",
    "            from spacy.lang.vi import Vietnamese as NlpTokenizer\n",
    "        elif region_code == 'ja':\n",
    "            from spacy.lang.ja import Japanese as NlpTokenizer\n",
    "        else:\n",
    "            raise Exception('Unknown region code: %s' % region_code)\n",
    "        nlp = NlpTokenizer()\n",
    "        nlp.add_pipe('sentencizer')\n",
    "    else:\n",
    "        nlp = spacy.load(dictionary_name)\n",
    "    \n",
    "    if basic_token:\n",
    "        return lambda text : [token.text for token in nlp(text)]\n",
    "    else:\n",
    "        return lambda text : [(token.text, token.lemma_, [token.idx, token.idx + len(token.text)]) for token in nlp(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_tokenizer(dictionary_name=None, region_code=None):\n",
    "    if region_code == 'vi':\n",
    "        from spacy.lang.vi import Vietnamese\n",
    "        nlp = Vietnamese()\n",
    "        nlp.add_pipe('sentencizer')\n",
    "    else:\n",
    "        nlp = spacy.load(dictionary_name)\n",
    "    \n",
    "    return lambda text : [sent.text.strip() for sent in nlp(text).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = ''.join(x for x in text if x not in set(string.punctuation))\n",
    "    return text.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('¡', '¡', [0, 1]),\n",
       " ('Hola', 'Hola', [1, 5]),\n",
       " ('mundo', 'mundo', [6, 11]),\n",
       " ('!', '!', [11, 12]),\n",
       " ('¡', '¡', [13, 14]),\n",
       " ('Adiós', 'Adiós', [14, 19]),\n",
       " ('mundo', 'mundo', [20, 25]),\n",
       " ('!', '!', [25, 26])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '¡Hola mundo! ¡Adiós mundo!'\n",
    "region_code = 'es'\n",
    "\n",
    "dictionary_name = LANGS[region_code]['spacy_dict']\n",
    "tokenizer = get_word_tokenizer(dictionary_name=dictionary_name, region_code=region_code)\n",
    "tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hola mundo.', '¡Adiós mundo!']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Hola mundo. ¡Adiós mundo!'\n",
    "region_code = 'es'\n",
    "\n",
    "dictionary_name = LANGS[region_code]['spacy_dict']\n",
    "tokenizer = get_sent_tokenizer(dictionary_name=dictionary_name, region_code=region_code)\n",
    "tokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_shannon(boe):\n",
    "    total = sum(boe.values())\n",
    "    return -1 * sum([freq / total * log2(freq / total) for freq in boe.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709505944546686"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'this car this car this'\n",
    "region_code = 'es'\n",
    "\n",
    "tokenizer = get_word_tokenizer(dictionary_name=dictionary_name, region_code=region_code)\n",
    "text = normalize_text(text)\n",
    "words = [x[0] for x in tokenizer(text)]\n",
    "boe = collections.Counter(words)\n",
    "\n",
    "entropy_shannon(boe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemPicker():\n",
    "    def __init__(self, dataset, timeout=60, sleep_interval=1):\n",
    "        self._n_items = len(dataset)\n",
    "        self._dataset_keys = list(dataset.keys())\n",
    "        self._dataset_values = list(dataset.values())\n",
    "        self._idx = 0\n",
    "        self._timeout = timeout\n",
    "        self._sleep_interval = sleep_interval\n",
    "        self._locked = False\n",
    "    \n",
    "    def pick(self):\n",
    "        self.lock()\n",
    "        if self._idx >= self._n_items:\n",
    "            item_name = None\n",
    "            item_props = None\n",
    "        else:\n",
    "            item_name = self._dataset_keys.pop(0)\n",
    "            item_props = self._dataset_values.pop(0)\n",
    "            self._idx += 1\n",
    "        self.unlock()\n",
    "        return item_name, item_props\n",
    "\n",
    "    def lock(self):\n",
    "        start_time = time.time()\n",
    "        while self._locked:\n",
    "            end_time = time.time()\n",
    "            if end_time - start_time >= self._timeout:\n",
    "                raise Exception('Cannot pick an item (timeout)')\n",
    "            sleep(self._sleep_interval)\n",
    "        self._locked = True\n",
    "    \n",
    "    def unlock(self):\n",
    "        self._locked = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset_by_entropy(input_file, output_file, region_code,\n",
    "                              min_word_entropy, max_word_entropy, verbose=False):\n",
    "    dictionary_name = LANGS[region_code]['spacy_dict']\n",
    "    stats = {\n",
    "        'n_words': [],\n",
    "        'entropy_shannon_chars': [],\n",
    "        'entropy_shannon_words': [],\n",
    "        'n_words_ps': [],\n",
    "        'entropy_shannon_chars_ps': [],\n",
    "        'entropy_shannon_words_ps': [],\n",
    "    }\n",
    "\n",
    "    basic_tokenizer = get_word_tokenizer(dictionary_name=dictionary_name, region_code=region_code, basic_token=True)\n",
    "    sent_tokenizer = get_sent_tokenizer(dictionary_name=dictionary_name, region_code=region_code)\n",
    "\n",
    "    with open(input_file, 'r', encoding='utf8') as fp:\n",
    "        dataset = json.load(fp)\n",
    "\n",
    "    if verbose:\n",
    "        print('Filtering elements...')\n",
    "    \n",
    "    filtered_dataset = {'version': 'filtered_%s' % dataset['version'], 'data': []}\n",
    "    for i, item in enumerate(dataset['data']):\n",
    "        if verbose:\n",
    "            print('- Item %d / %d' % (i + 1, len(dataset['data'])), end='\\r')\n",
    "        \n",
    "        for paragraph_item in item['paragraphs']:\n",
    "            context = paragraph_item['context']\n",
    "            words = basic_tokenizer(normalize_text(context))\n",
    "            freq_words = collections.Counter(words)\n",
    "            words_entropy = entropy_shannon(freq_words)\n",
    "            \n",
    "            if words_entropy < min_word_entropy or words_entropy > max_word_entropy:\n",
    "                continue\n",
    "            \n",
    "            filtered_dataset['data'].append({\n",
    "                'title': item['title'],\n",
    "                'paragraphs': [\n",
    "                    {\n",
    "                        'context': paragraph_item['context'],\n",
    "                        'qas': paragraph_item['qas'],\n",
    "                    }\n",
    "                ]\n",
    "            })\n",
    "    if verbose:\n",
    "        print()\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf8') as fp:\n",
    "        json.dump(filtered_dataset, fp)\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thread_main(end_status, item_picker, min_word_entropy, max_word_entropy):\n",
    "    while True:\n",
    "        dataset_name, dataset_props = item_picker.pick()\n",
    "        if dataset_name is None:\n",
    "            break\n",
    "        print('Processing dataset: %s' % dataset_name)\n",
    "        input_file, output_file, region_code = dataset_props\n",
    "        filtered_dataset = filter_dataset_by_entropy(\n",
    "                input_file, output_file, region_code, min_word_entropy, max_word_entropy)\n",
    "        print('Finished dataset: %s' % dataset_name)\n",
    "    end_status.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'synthetic-es-top1': [\n",
    "        '../data/synthetic_google_top_n/es/top_1/train-synthetic.json',\n",
    "        '../data/synthetic_google_top_n/es/top_1/filtered-train-synthetic.json',\n",
    "        'es'\n",
    "    ],\n",
    "    'synthetic-ru-top1': [\n",
    "        '../data/synthetic_google_top_n/ru/top_1/train-synthetic.json',\n",
    "        '../data/synthetic_google_top_n/ru/top_1/filtered-train-synthetic.json',\n",
    "        'ru'\n",
    "    ],\n",
    "    'synthetic-vi-top1': [\n",
    "        '../data/synthetic_google_top_n/vi/top_1/train-synthetic.json',\n",
    "        '../data/synthetic_google_top_n/vi/top_1/filtered-train-synthetic.json',\n",
    "        'vi'\n",
    "    ],\n",
    "    'synthetic-ja-top1': [\n",
    "        '../data/synthetic_google_top_n/ja/top_1/train-synthetic.json',\n",
    "        '../data/synthetic_google_top_n/ja/top_1/filtered-train-synthetic.json',\n",
    "        'ja'\n",
    "    ],\n",
    "    'synthetic-es-top2': [\n",
    "        '../data/synthetic_google_top_n/es/top_2/train-synthetic.json',\n",
    "        '../data/synthetic_google_top_n/es/top_2/filtered-train-synthetic.json',\n",
    "        'es'\n",
    "    ],\n",
    "    'synthetic-ru-top2': [\n",
    "        '../data/synthetic_google_top_n/ru/top_2/train-synthetic.json',\n",
    "        '../data/synthetic_google_top_n/ru/top_2/filtered-train-synthetic.json',\n",
    "        'ru'\n",
    "    ],\n",
    "    'synthetic-vi-top2': [\n",
    "        '../data/synthetic_google_top_n/vi/top_2/train-synthetic.json',\n",
    "        '../data/synthetic_google_top_n/vi/top_2/filtered-train-synthetic.json',\n",
    "        'vi'\n",
    "    ],\n",
    "    'synthetic-ja-top2': [\n",
    "        '../data/synthetic_google_top_n/ja/top_2/train-synthetic.json',\n",
    "        '../data/synthetic_google_top_n/ja/top_2/filtered-train-synthetic.json',\n",
    "        'ja'\n",
    "    ],\n",
    "    'synthetic-es-top3': [\n",
    "        '../data/synthetic_google_top_n/es/top_3/train-synthetic.json',\n",
    "        '../data/synthetic_google_top_n/es/top_3/filtered-train-synthetic.json',\n",
    "        'es'\n",
    "    ],\n",
    "    'synthetic-ru-top3': [\n",
    "        '../data/synthetic_google_top_n/ru/top_3/train-synthetic.json',\n",
    "        '../data/synthetic_google_top_n/ru/top_3/filtered-train-synthetic.json',\n",
    "        'ru'\n",
    "    ],\n",
    "    'synthetic-vi-top3': [\n",
    "        '../data/synthetic_google_top_n/vi/top_3/train-synthetic.json',\n",
    "        '../data/synthetic_google_top_n/vi/top_3/filtered-train-synthetic.json',\n",
    "        'vi'\n",
    "    ],\n",
    "    'synthetic-ja-top3': [\n",
    "        '../data/synthetic_google_top_n/ja/top_3/train-synthetic.json',\n",
    "        '../data/synthetic_google_top_n/ja/top_3/filtered-train-synthetic.json',\n",
    "        'ja'\n",
    "    ],\n",
    "    'synthetic-es-top5': [\n",
    "        '../data/synthetic_google_top_n/es/top_5/train-synthetic.json',\n",
    "        '../data/synthetic_google_top_n/es/top_5/filtered-train-synthetic.json',\n",
    "        'es'\n",
    "    ],\n",
    "    'synthetic-ru-top5': [\n",
    "        '../data/synthetic_google_top_n/ru/top_5/train-synthetic.json',\n",
    "        '../data/synthetic_google_top_n/ru/top_5/filtered-train-synthetic.json',\n",
    "        'ru'\n",
    "    ],\n",
    "    'synthetic-vi-top5': [\n",
    "        '../data/synthetic_google_top_n/vi/top_5/train-synthetic.json',\n",
    "        '../data/synthetic_google_top_n/vi/top_5/filtered-train-synthetic.json',\n",
    "        'vi'\n",
    "    ],\n",
    "    'synthetic-ja-top5': [\n",
    "        '../data/synthetic_google_top_n/ja/top_5/train-synthetic.json',\n",
    "        '../data/synthetic_google_top_n/ja/top_5/filtered-train-synthetic.json',\n",
    "        'ja'\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: synthetic-vi-top1\n",
      "Processing dataset: synthetic-vi-top2\n",
      "Processing dataset: synthetic-vi-top3\n",
      "Processing dataset: synthetic-vi-top5\n",
      "Finished dataset: synthetic-vi-top1\n",
      "Finished 1 / 10 threads...\n",
      "Finished dataset: synthetic-vi-top2\n",
      "Finished 2 / 10 threads...\n",
      "Finished dataset: synthetic-vi-top3\n",
      "Finished 3 / 10 threads...\n",
      "Finished dataset: synthetic-vi-top5\n",
      "Finished 4 / 10 threads...\n",
      "Finished 5 / 10 threads...\n",
      "Finished 6 / 10 threads...\n",
      "Finished 7 / 10 threads...\n",
      "Finished 8 / 10 threads...\n",
      "Finished 9 / 10 threads...\n",
      "Finished 10 / 10 threads...\n"
     ]
    }
   ],
   "source": [
    "item_picker = ItemPicker(datasets)\n",
    "n_threads = 10\n",
    "\n",
    "min_word_entropy = 5\n",
    "max_word_entropy = 7\n",
    "\n",
    "all_main_threads = []\n",
    "end_status = []\n",
    "for _ in range(n_threads):\n",
    "    x = threading.Thread(\n",
    "        target=thread_main,\n",
    "        args=(end_status, item_picker, min_word_entropy, max_word_entropy))\n",
    "    x.start()\n",
    "    all_main_threads.append(x)\n",
    "    sleep(1)\n",
    "\n",
    "for i, x in enumerate(all_main_threads):\n",
    "    x.join()\n",
    "    print('Finished %d / %d threads...' % (i + 1, n_threads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

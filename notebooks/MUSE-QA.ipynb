{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MUSE\n",
    "\n",
    "Download multilingual word embeddings from a source language into English and store them into `./data/muse/`.\n",
    "\n",
    "See: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import os\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from src import *\n",
    "from src.data import *\n",
    "from src.data.dataset import *\n",
    "from src.data.squad import *\n",
    "from src.data.tokenizers import *\n",
    "from src.models.metrics import *\n",
    "from src.models.qa import *\n",
    "from src.utils.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = None\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SEED = 42\n",
    "set_seed(DEFAULT_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data'\n",
    "ARTIFACTS_PATH = '../artifacts'\n",
    "\n",
    "LANG_TRAIN = 'en'\n",
    "TRAIN_FILE = os.path.join(DATA_PATH, 'squad/train-v1.1-small-5k.json')\n",
    "\n",
    "LANG_DEV = 'en'\n",
    "DEV_FILE = os.path.join(DATA_PATH, 'squad/dev-v1.1.json')\n",
    "\n",
    "SPACY_DICT = {\n",
    "    'en': 'en_core_web_sm',\n",
    "    'es': 'es_core_news_sm',\n",
    "    'ru': 'ru_core_news_sm',\n",
    "    'vi': 'vi',\n",
    "}\n",
    "\n",
    "# Tokenization\n",
    "MAX_PADDING = 512\n",
    "SEP_TOKEN = '[SEP]'\n",
    "PAD_TOKEN = '[PAD]'\n",
    "UNKNOWN_TOKEN = '[UNK]'\n",
    "\n",
    "# Training settings\n",
    "CKPT_NAME = 'qa_muse_squad5k'\n",
    "BATCH_SIZE = 32\n",
    "MAX_EPOCHES = 25\n",
    "LR_VALUE = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embeddings and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(lang_code, nmax=1000000, data_path=DATA_PATH,\n",
    "                    unknown_token=UNKNOWN_TOKEN, sep_token=SEP_TOKEN, pad_token=PAD_TOKEN):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    emb_path = os.path.join(data_path, 'muse', 'wiki.multi.%s.vec' % lang_code)\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ', dtype='float32')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax - 3:\n",
    "                break\n",
    "    \n",
    "    # Add token to separate items\n",
    "    vect_random = np.ones(vectors[-1].shape[-1], dtype='float32')\n",
    "    vectors.append(vect_random)\n",
    "    word2id[sep_token] = len(word2id)\n",
    "    \n",
    "    # Add token for padding\n",
    "    vect_random = np.zeros(vectors[-1].shape[-1], dtype='float32')\n",
    "    vectors.append(vect_random)\n",
    "    word2id[pad_token] = len(word2id)\n",
    "    \n",
    "    # Add token for unknown words\n",
    "    vect_random = np.random.random(vectors[-1].shape[-1]).astype('float32')\n",
    "    vectors.append(vect_random)\n",
    "    word2id[unknown_token] = len(word2id)\n",
    "    \n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_tokenizer(nlp, word2id, context, question=None, max_padding=MAX_PADDING,\n",
    "                     unknown_token=UNKNOWN_TOKEN, sep_token=SEP_TOKEN, pad_token=PAD_TOKEN):\n",
    "    data = {'input_ids': [], 'token_type_ids': [], 'attention_mask': [], 'offset_mapping': []}\n",
    "    \n",
    "    # Context data\n",
    "    for token in nlp(context):\n",
    "        token_id = word2id[token.text] if token.text in word2id else word2id[unknown_token]\n",
    "        data['input_ids'].append(token_id)\n",
    "        data['token_type_ids'].append(0)\n",
    "        data['attention_mask'].append(1)\n",
    "        data['offset_mapping'].append([token.idx, token.idx + len(token.text)])\n",
    "    \n",
    "    # Question data\n",
    "    if question:\n",
    "        data['input_ids'].append(word2id[sep_token])\n",
    "        data['token_type_ids'].append(1)\n",
    "        data['attention_mask'].append(0)\n",
    "        data['offset_mapping'].append([0, 0])\n",
    "        for token in nlp(context):\n",
    "            token_id = word2id[token.text] if token.text in word2id else word2id[unknown_token]\n",
    "            data['input_ids'].append(token_id)\n",
    "            data['token_type_ids'].append(1)\n",
    "            data['attention_mask'].append(0)\n",
    "            data['offset_mapping'].append([token.idx, token.idx + len(token.text)])\n",
    "    \n",
    "    # Padding\n",
    "    if len(data['input_ids']) < max_padding:\n",
    "        for _ in range(max_padding - len(data['input_ids'])):\n",
    "            data['input_ids'].append(word2id[pad_token])\n",
    "            data['token_type_ids'].append(0)\n",
    "            data['attention_mask'].append(0)\n",
    "            data['offset_mapping'].append([0, 0])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_tokenizer(dictionary_name, word2id):\n",
    "    if dictionary_name == 'vi':\n",
    "        nlp = Vietnamese()\n",
    "    else:\n",
    "        nlp = spacy.load(dictionary_name)\n",
    "    return lambda context, question=None, **kwargs : lambda_tokenizer(nlp, word2id, context, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_embeddings, train_id2word, train_word2id = load_embeddings(LANG_TRAIN)\n",
    "train_word_tokenizer = get_word_tokenizer(SPACY_DICT[LANG_TRAIN], train_word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_word_embeddings, dev_id2word, dev_word2id = load_embeddings(LANG_TRAIN)\n",
    "dev_word_tokenizer = get_word_tokenizer(SPACY_DICT[LANG_DEV], dev_word2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_path = TRAIN_FILE\n",
    "dataset_dev_path = DEV_FILE\n",
    "\n",
    "if not os.path.exists(dataset_train_path):\n",
    "    raise Exception('Train dataset does not exist: %s' % dataset_train_path)\n",
    "elif not os.path.exists(dataset_dev_path):\n",
    "    raise Exception('Dev dataset does not exist: %s' % dataset_dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading train dataset: %s' % dataset_train_path)\n",
    "train_squad_preprocess = SquadPreprocess(train_word_tokenizer, max_length=MAX_PADDING)\n",
    "train_dataset = SquadDataset(train_squad_preprocess, dataset_train_path, save_contexts=False)\n",
    "train_skipped = train_dataset.get_skipped_items()\n",
    "print('- Train data: %d (skipped: %d)' % (len(train_dataset), len(train_skipped)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading dev dataset: %s' % dataset_dev_path)\n",
    "dev_squad_preprocess = SquadPreprocess(dev_word_tokenizer, max_length=MAX_PADDING)\n",
    "dev_dataset = SquadDataset(dev_squad_preprocess, dataset_dev_path)\n",
    "dev_skipped = dev_dataset.get_skipped_items()\n",
    "print('- Dev data: %d (skipped: %d)' % (len(dev_dataset), len(dev_skipped)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating data loaders...')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define&build MuseQA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MuseQA(nn.Module):\n",
    "    def __init__(self, word_embeddings, max_length=512, device=None):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        hidden_size = word_embeddings[-1].shape[-1]\n",
    "        word_embeddings = torch.from_numpy(word_embeddings)\n",
    "        self.emb_layer = nn.Embedding.from_pretrained(word_embeddings, freeze=True).to(device=device)\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size, bias=False).to(device=device)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.start_span = nn.Linear(hidden_size, 1, bias=False).to(device=device)\n",
    "        self.end_span = nn.Linear(hidden_size, 1, bias=False).to(device=device)\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, start_positions=None, end_positions=None, **kwargs):\n",
    "        x = self.emb_layer(input_ids)\n",
    "        x = torch.relu(self.hidden_layer(x))\n",
    "\n",
    "        x_start = torch.relu(self.start_span(x))\n",
    "        x_start = self.flatten(x_start)\n",
    "        x_start = attention_mask * x_start\n",
    "        \n",
    "        x_end = torch.relu(self.end_span(x))\n",
    "        x_end = self.flatten(x_end)\n",
    "        x_end = attention_mask * x_end\n",
    "        \n",
    "        if start_positions is None or end_positions is None:\n",
    "            return x_start, x_end\n",
    "        else:\n",
    "            loss_start = self.criterion(x_start, start_positions)\n",
    "            loss_end = self.criterion(x_end, end_positions)\n",
    "            loss = loss_start + loss_end\n",
    "            return loss, x_start, x_end\n",
    "    \n",
    "    def get_top_weights(self):\n",
    "        return self.hidden_layer.weight.data, self.start_span.weight.data, self.end_span.weight.data\n",
    "    \n",
    "    def set_top_weights(self, hidden_layer_weights, start_span_weights, end_span_weights):\n",
    "        self.hidden_layer.weight.data = hidden_layer_weights\n",
    "        self.start_span.weight.data = start_span_weights\n",
    "        self.end_span.weight.data = end_span_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MuseModelManager(ModelManager):\n",
    "    def __init__(self):\n",
    "        ModelManager.__init__(self)\n",
    "\n",
    "    def build(self, device=None, **kwargs):\n",
    "        model = MuseQA(kwargs['word_embeddings'], kwargs['word2id'], device=device)\n",
    "        model.to(device)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = MuseModelManager()\n",
    "model = manager.build(word_embeddings=train_word_embeddings, word2id=train_word2id, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = list(train_dataset.idx2pos.keys())[0]\n",
    "\n",
    "sample_data = train_dataset.get_item(sample_idx)\n",
    "x_input_ids = torch.unsqueeze(sample_data['input_ids'], 0).to(device=device)\n",
    "x_attention_mask = torch.unsqueeze(sample_data['attention_mask'], 0).to(device=device)\n",
    "x_start_token_idx = torch.unsqueeze(sample_data['start_token_idx'], 0).to(device=device)\n",
    "x_end_token_idx = torch.unsqueeze(sample_data['end_token_idx'], 0).to(device=device)\n",
    "\n",
    "loss, outputs1, outputs2 = model(x_input_ids, x_attention_mask, x_start_token_idx, x_end_token_idx)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    cased=True,\n",
    "    model_type='muse',\n",
    "    ckpt_name=CKPT_NAME,\n",
    "    dataset_train_path=TRAIN_FILE,\n",
    "    dataset_train_lang=LANG_TRAIN,\n",
    "    dataset_dev_path=DEV_FILE,\n",
    "    dataset_dev_lang=LANG_DEV,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_epoches=MAX_EPOCHES,\n",
    "    max_length=MAX_PADDING,\n",
    "    learning_rate=LR_VALUE,\n",
    "    continue_training=False,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have two models since source language might be different to the target one\n",
    "manager = MuseModelManager()\n",
    "train_model = manager.build(word_embeddings=train_word_embeddings, word2id=train_word2id, device=device)\n",
    "dev_model = manager.build(word_embeddings=dev_word_embeddings, word2id=dev_word2id, device=device)\n",
    "\n",
    "train_model.train()\n",
    "dev_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_exact_match = ExactMatch(dev_dataset, device=config.device)\n",
    "dev_exact_match = ExactMatch(dev_dataset, device=config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(train_model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = get_project_path('artifacts', config.ckpt_name)\n",
    "current_epoch = 0\n",
    "current_score = 0.\n",
    "best_score = 0.\n",
    "n_batches = len(train_dataloader)\n",
    "\n",
    "for _ in range(config.max_epoches):\n",
    "    run_loss = 0.\n",
    "    i_epoch = config.current_epoch\n",
    "    config.current_epoch += 1\n",
    "    \n",
    "    train_model.train()\n",
    "\n",
    "    for i_batch, batch_data in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get inputs\n",
    "        input_ids = batch_data['input_ids'].to(device=config.device)\n",
    "        attention_mask = batch_data['attention_mask'].to(device=config.device)\n",
    "        start_token_idx = batch_data['start_token_idx'].to(device=config.device)\n",
    "        end_token_idx = batch_data['end_token_idx'].to(device=config.device)\n",
    "\n",
    "        # Inference\n",
    "        loss, outputs1, outputs2 = train_model(input_ids=input_ids,\n",
    "                                               attention_mask=attention_mask,\n",
    "                                               start_positions=start_token_idx,\n",
    "                                               end_positions=end_token_idx)\n",
    "\n",
    "        # Compute loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        run_loss += loss.cpu().data.numpy()\n",
    "\n",
    "        if i_batch % 50 == 0:\n",
    "            print(\"Epoch %d of %d | Batch %d of %d | Loss = %.3f\" % (\n",
    "                    i_epoch + 1, config.max_epoches, i_batch + 1, n_batches, run_loss / (i_batch + 1)))\n",
    "\n",
    "        # Clear some memory\n",
    "        if config.device == 'cuda':\n",
    "            del input_ids\n",
    "            del attention_mask\n",
    "            del start_token_idx\n",
    "            del end_token_idx\n",
    "            del outputs1\n",
    "            del outputs2\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"Epoch %d of %d | Loss = %.3f\" % (i_epoch + 1, config.max_epoches,\n",
    "                                             run_loss / len(train_dataloader)))\n",
    "\n",
    "    print('Evaluating model...')\n",
    "    hidden_layer_weights, start_span_weights, end_span_weights = train_model.get_top_weights()\n",
    "    dev_model.set_top_weights(hidden_layer_weights, start_span_weights, end_span_weights)\n",
    "    dev_score = dev_exact_match.eval(dev_model)\n",
    "\n",
    "    print('Dev Score: %.4f | Best: %.4f' % (dev_score, best_score))\n",
    "    \n",
    "    train_model.eval()\n",
    "    train_score = train_exact_match.eval(train_model)\n",
    "    print('Train Score: %.4f' % (train_score))\n",
    "\n",
    "    if dev_score > best_score:\n",
    "        print('Score Improved! Saving model...')\n",
    "        best_score = dev_score\n",
    "        config.current_score = best_score\n",
    "        manager.save(train_model, config, save_path)\n",
    "\n",
    "print('End training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

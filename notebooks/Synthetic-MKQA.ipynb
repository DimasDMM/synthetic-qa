{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic SQuAD dataset based on MKQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import math\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import urllib\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM_NAME = 'bert-base-multilingual-cased'\n",
    "INPUT_FILE = '../data/mkqa/mkqa.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(INPUT_FILE, 'r', encoding='utf-8') as fp:\n",
    "    mkqa_dataset = list(fp)\n",
    "\n",
    "mkqa_dataset = [json.loads(jline) for jline in mkqa_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_num(text):\n",
    "    try:\n",
    "        return int(text)\n",
    "    except:\n",
    "        return float(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBPedia\n",
    "\n",
    "In order to get the Wiki entities in the questions, we will use DBPedia. This service will run in a docker container.\n",
    "\n",
    "Download the models from https://databus.dbpedia.org/dbpedia/spotlight/spotlight-model/\n",
    "\n",
    "See:\n",
    "- https://github.com/dbpedia-spotlight/dbpedia-spotlight\n",
    "- https://www.dbpedia-spotlight.org/api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'al',\n",
       " 'algo',\n",
       " 'algunas',\n",
       " 'algunos',\n",
       " 'ante',\n",
       " 'antes',\n",
       " 'como',\n",
       " 'con',\n",
       " 'contra',\n",
       " 'cual',\n",
       " 'cuando',\n",
       " 'de',\n",
       " 'del',\n",
       " 'desde',\n",
       " 'donde',\n",
       " 'durante',\n",
       " 'e',\n",
       " 'el',\n",
       " 'ella',\n",
       " 'ellas',\n",
       " 'ellos',\n",
       " 'en',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'erais',\n",
       " 'eran',\n",
       " 'eras',\n",
       " 'eres',\n",
       " 'es',\n",
       " 'esa',\n",
       " 'esas',\n",
       " 'ese',\n",
       " 'eso',\n",
       " 'esos',\n",
       " 'esta',\n",
       " 'estaba',\n",
       " 'estabais',\n",
       " 'estaban',\n",
       " 'estabas',\n",
       " 'estad',\n",
       " 'estada',\n",
       " 'estadas',\n",
       " 'estado',\n",
       " 'estados',\n",
       " 'estamos',\n",
       " 'estando',\n",
       " 'estar',\n",
       " 'estaremos',\n",
       " 'estará',\n",
       " 'estarán',\n",
       " 'estarás',\n",
       " 'estaré',\n",
       " 'estaréis',\n",
       " 'estaría',\n",
       " 'estaríais',\n",
       " 'estaríamos',\n",
       " 'estarían',\n",
       " 'estarías',\n",
       " 'estas',\n",
       " 'este',\n",
       " 'estemos',\n",
       " 'esto',\n",
       " 'estos',\n",
       " 'estoy',\n",
       " 'estuve',\n",
       " 'estuviera',\n",
       " 'estuvierais',\n",
       " 'estuvieran',\n",
       " 'estuvieras',\n",
       " 'estuvieron',\n",
       " 'estuviese',\n",
       " 'estuvieseis',\n",
       " 'estuviesen',\n",
       " 'estuvieses',\n",
       " 'estuvimos',\n",
       " 'estuviste',\n",
       " 'estuvisteis',\n",
       " 'estuviéramos',\n",
       " 'estuviésemos',\n",
       " 'estuvo',\n",
       " 'está',\n",
       " 'estábamos',\n",
       " 'estáis',\n",
       " 'están',\n",
       " 'estás',\n",
       " 'esté',\n",
       " 'estéis',\n",
       " 'estén',\n",
       " 'estés',\n",
       " 'fue',\n",
       " 'fuera',\n",
       " 'fuerais',\n",
       " 'fueran',\n",
       " 'fueras',\n",
       " 'fueron',\n",
       " 'fuese',\n",
       " 'fueseis',\n",
       " 'fuesen',\n",
       " 'fueses',\n",
       " 'fui',\n",
       " 'fuimos',\n",
       " 'fuiste',\n",
       " 'fuisteis',\n",
       " 'fuéramos',\n",
       " 'fuésemos',\n",
       " 'ha',\n",
       " 'habida',\n",
       " 'habidas',\n",
       " 'habido',\n",
       " 'habidos',\n",
       " 'habiendo',\n",
       " 'habremos',\n",
       " 'habrá',\n",
       " 'habrán',\n",
       " 'habrás',\n",
       " 'habré',\n",
       " 'habréis',\n",
       " 'habría',\n",
       " 'habríais',\n",
       " 'habríamos',\n",
       " 'habrían',\n",
       " 'habrías',\n",
       " 'habéis',\n",
       " 'había',\n",
       " 'habíais',\n",
       " 'habíamos',\n",
       " 'habían',\n",
       " 'habías',\n",
       " 'han',\n",
       " 'has',\n",
       " 'hasta',\n",
       " 'hay',\n",
       " 'haya',\n",
       " 'hayamos',\n",
       " 'hayan',\n",
       " 'hayas',\n",
       " 'hayáis',\n",
       " 'he',\n",
       " 'hemos',\n",
       " 'hube',\n",
       " 'hubiera',\n",
       " 'hubierais',\n",
       " 'hubieran',\n",
       " 'hubieras',\n",
       " 'hubieron',\n",
       " 'hubiese',\n",
       " 'hubieseis',\n",
       " 'hubiesen',\n",
       " 'hubieses',\n",
       " 'hubimos',\n",
       " 'hubiste',\n",
       " 'hubisteis',\n",
       " 'hubiéramos',\n",
       " 'hubiésemos',\n",
       " 'hubo',\n",
       " 'la',\n",
       " 'las',\n",
       " 'le',\n",
       " 'les',\n",
       " 'lo',\n",
       " 'los',\n",
       " 'me',\n",
       " 'mi',\n",
       " 'mis',\n",
       " 'mucho',\n",
       " 'muchos',\n",
       " 'muy',\n",
       " 'más',\n",
       " 'mí',\n",
       " 'mía',\n",
       " 'mías',\n",
       " 'mío',\n",
       " 'míos',\n",
       " 'nada',\n",
       " 'ni',\n",
       " 'no',\n",
       " 'nos',\n",
       " 'nosotras',\n",
       " 'nosotros',\n",
       " 'nuestra',\n",
       " 'nuestras',\n",
       " 'nuestro',\n",
       " 'nuestros',\n",
       " 'o',\n",
       " 'os',\n",
       " 'otra',\n",
       " 'otras',\n",
       " 'otro',\n",
       " 'otros',\n",
       " 'para',\n",
       " 'pero',\n",
       " 'poco',\n",
       " 'por',\n",
       " 'porque',\n",
       " 'que',\n",
       " 'quien',\n",
       " 'quienes',\n",
       " 'qué',\n",
       " 'se',\n",
       " 'sea',\n",
       " 'seamos',\n",
       " 'sean',\n",
       " 'seas',\n",
       " 'sentid',\n",
       " 'sentida',\n",
       " 'sentidas',\n",
       " 'sentido',\n",
       " 'sentidos',\n",
       " 'seremos',\n",
       " 'será',\n",
       " 'serán',\n",
       " 'serás',\n",
       " 'seré',\n",
       " 'seréis',\n",
       " 'sería',\n",
       " 'seríais',\n",
       " 'seríamos',\n",
       " 'serían',\n",
       " 'serías',\n",
       " 'seáis',\n",
       " 'siente',\n",
       " 'sin',\n",
       " 'sintiendo',\n",
       " 'sobre',\n",
       " 'sois',\n",
       " 'somos',\n",
       " 'son',\n",
       " 'soy',\n",
       " 'su',\n",
       " 'sus',\n",
       " 'suya',\n",
       " 'suyas',\n",
       " 'suyo',\n",
       " 'suyos',\n",
       " 'sí',\n",
       " 'también',\n",
       " 'tanto',\n",
       " 'te',\n",
       " 'tendremos',\n",
       " 'tendrá',\n",
       " 'tendrán',\n",
       " 'tendrás',\n",
       " 'tendré',\n",
       " 'tendréis',\n",
       " 'tendría',\n",
       " 'tendríais',\n",
       " 'tendríamos',\n",
       " 'tendrían',\n",
       " 'tendrías',\n",
       " 'tened',\n",
       " 'tenemos',\n",
       " 'tenga',\n",
       " 'tengamos',\n",
       " 'tengan',\n",
       " 'tengas',\n",
       " 'tengo',\n",
       " 'tengáis',\n",
       " 'tenida',\n",
       " 'tenidas',\n",
       " 'tenido',\n",
       " 'tenidos',\n",
       " 'teniendo',\n",
       " 'tenéis',\n",
       " 'tenía',\n",
       " 'teníais',\n",
       " 'teníamos',\n",
       " 'tenían',\n",
       " 'tenías',\n",
       " 'ti',\n",
       " 'tiene',\n",
       " 'tienen',\n",
       " 'tienes',\n",
       " 'todo',\n",
       " 'todos',\n",
       " 'tu',\n",
       " 'tus',\n",
       " 'tuve',\n",
       " 'tuviera',\n",
       " 'tuvierais',\n",
       " 'tuvieran',\n",
       " 'tuvieras',\n",
       " 'tuvieron',\n",
       " 'tuviese',\n",
       " 'tuvieseis',\n",
       " 'tuviesen',\n",
       " 'tuvieses',\n",
       " 'tuvimos',\n",
       " 'tuviste',\n",
       " 'tuvisteis',\n",
       " 'tuviéramos',\n",
       " 'tuviésemos',\n",
       " 'tuvo',\n",
       " 'tuya',\n",
       " 'tuyas',\n",
       " 'tuyo',\n",
       " 'tuyos',\n",
       " 'tú',\n",
       " 'un',\n",
       " 'una',\n",
       " 'uno',\n",
       " 'unos',\n",
       " 'vosotras',\n",
       " 'vosotros',\n",
       " 'vuestra',\n",
       " 'vuestras',\n",
       " 'vuestro',\n",
       " 'vuestros',\n",
       " 'y',\n",
       " 'ya',\n",
       " 'yo',\n",
       " 'él',\n",
       " 'éramos'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('spanish'))\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dbpedia_annotations(text, min_score=0.95, lang_name='spanish'):\n",
    "    text = urllib.parse.quote_plus(text)\n",
    "    url = 'http://127.0.0.1:8080/rest/annotate' + \\\n",
    "        '?text=%s' % text + \\\n",
    "        '&confidence=0'\n",
    "    \n",
    "    r = requests.get(url, headers={'Accept': 'application/json'})\n",
    "    if r.status_code != 200:\n",
    "        print('')\n",
    "        print('Unexpected HTTP Status: %d - %s' % (r.status_code, url))\n",
    "        #raise Exception('Unexpected HTTP Status: %d' % r.status_code)\n",
    "        return []\n",
    "    \n",
    "    raw_data = r.json()\n",
    "    \n",
    "    resources = []\n",
    "    \n",
    "    if 'Resources' not in raw_data:\n",
    "        return []\n",
    "    \n",
    "    stopwords = set(nltk.corpus.stopwords.words(lang_name))\n",
    "    \n",
    "    for item in raw_data['Resources']:\n",
    "        item_score = str_to_num(item['@similarityScore'])\n",
    "        if item_score >= min_score and item['@surfaceForm'] not in stopwords:\n",
    "            resources.append({\n",
    "                'uri': item['@URI'],\n",
    "                'text': item['@surfaceForm'],\n",
    "                'score': item_score,\n",
    "                'types': item['@types'],\n",
    "                'support': item['@support'],\n",
    "            })\n",
    "\n",
    "    return resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'uri': 'http://es.dbpedia.org/resource/Ganado',\n",
       "  'text': 'ganado',\n",
       "  'score': 0.9710442510960338,\n",
       "  'types': '',\n",
       "  'support': '1721'},\n",
       " {'uri': 'http://es.dbpedia.org/resource/Mayor',\n",
       "  'text': 'mayor',\n",
       "  'score': 0.9643462383456692,\n",
       "  'types': '',\n",
       "  'support': '2545'},\n",
       " {'uri': 'http://es.dbpedia.org/resource/Copa_del_Mundo_de_Tenis_de_Mesa',\n",
       "  'text': 'Copa del mundo',\n",
       "  'score': 0.9927741286808874,\n",
       "  'types': '',\n",
       "  'support': '41'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = 'Qué país ha ganado el mayor número de títulos de Copa del mundo'\n",
    "sample_annotations = get_dbpedia_annotations(sample_text)\n",
    "sample_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'uri': 'http://es.dbpedia.org/resource/Bill_Gates',\n",
       "  'text': 'Bill Gates',\n",
       "  'score': 0.9999999999940457,\n",
       "  'types': 'Http://xmlns.com/foaf/0.1/Person,Wikidata:Q5,Wikidata:Q24229398,Wikidata:Q215627,DUL:NaturalPerson,DUL:Agent,Schema:Person,DBpedia:Agent,DBpedia:Person',\n",
       "  'support': '478'},\n",
       " {'uri': 'http://es.dbpedia.org/resource/Vive_(Venezuela)',\n",
       "  'text': 'vive',\n",
       "  'score': 0.9784932221938503,\n",
       "  'types': 'Wikidata:Q43229,Wikidata:Q24229398,Wikidata:Q15265344,DUL:SocialPerson,DUL:Agent,Schema:TelevisionStation,Schema:Organization,DBpedia:Organisation,DBpedia:Broadcaster,DBpedia:Agent,DBpedia:TelevisionStation',\n",
       "  'support': '45'},\n",
       " {'uri': 'http://es.dbpedia.org/resource/Estados_Unidos',\n",
       "  'text': 'Estados Unidos',\n",
       "  'score': 0.9999993981782999,\n",
       "  'types': 'Wikidata:Q6256,Schema:Place,Schema:Country,DBpedia:PopulatedPlace,DBpedia:Place,DBpedia:Location,DBpedia:Country',\n",
       "  'support': '399783'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = '¿Bill Gates vive en Estados Unidos?'\n",
    "sample_annotations = get_dbpedia_annotations(sample_text)\n",
    "sample_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dbpedia_entity_id(url):\n",
    "    r = requests.get(url, headers={'Accept': 'application/json'})\n",
    "    if r.status_code != 200:\n",
    "        print('')\n",
    "        print('Unexpected HTTP Status: %d - %s' % (r.status_code, url))\n",
    "        #raise Exception('Unexpected HTTP Status: %d' % r.status_code)\n",
    "        return None\n",
    "    \n",
    "    raw_data = r.json()\n",
    "    if url not in raw_data or 'http://dbpedia.org/ontology/wikiPageID' not in raw_data[url]:\n",
    "        return None\n",
    "    \n",
    "    wiki_id = raw_data[url]['http://dbpedia.org/ontology/wikiPageID'][0]['value']\n",
    "    \n",
    "    return wiki_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dbpedia_entity_id(sample_annotations[0]['uri'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dbpedia_search(query, min_score):\n",
    "    annotations = get_dbpedia_annotations(query, min_score=min_score)\n",
    "    entities = []\n",
    "    for item in annotations:\n",
    "        page_id = get_dbpedia_entity_id(item['uri'])\n",
    "        if page_id is None:\n",
    "            continue\n",
    "        entities.append({\n",
    "            'wiki_title': item['text'],\n",
    "            'wiki_page_id': page_id,\n",
    "            'entity_text': item['text'],\n",
    "            'entity_score': item['score'],\n",
    "            'is_mandatory': False,\n",
    "        })\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki API\n",
    "\n",
    "Wikipedia API:\n",
    "```\n",
    "https://en.wikipedia.org/w/api.php\n",
    "?action=query\n",
    "&list=search\n",
    "&srsearch=zyz        # search query\n",
    "&srlimit=1           # return only the first result\n",
    "&srnamespace=0       # search only articles, ignoring Talk, Mediawiki, etc.\n",
    "&format=json         # jsonfm prints the JSON in HTML for debugging.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_search(query, top_results, lang_code='es'):\n",
    "    query = urllib.parse.quote_plus(query)\n",
    "    url = 'https://%s.wikipedia.org/w/api.php' % lang_code + \\\n",
    "        '?action=query' + \\\n",
    "        '&list=search' + \\\n",
    "        '&srsearch=%s' % query + \\\n",
    "        '&srlimit=%d' % top_results + \\\n",
    "        '&srnamespace=0' + \\\n",
    "        '&format=json'\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    if r.status_code != 200:\n",
    "        print('')\n",
    "        print('Unexpected HTTP Status: %d - %s' % (r.status_code, url))\n",
    "        #raise Exception('Unexpected HTTP Status: %d' % r.status_code)\n",
    "        return []\n",
    "\n",
    "    raw_data = r.json()\n",
    "    \n",
    "    if 'query' not in raw_data:\n",
    "        return []\n",
    "    \n",
    "    items = []\n",
    "    for raw_item in raw_data['query']['search']:\n",
    "        items.append({\n",
    "            'wiki_title': raw_item['title'],\n",
    "            'wiki_page_id': raw_item['pageid'],\n",
    "        })\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'wiki_title': 'Medina (Washington)', 'wiki_page_id': 4384248},\n",
       " {'wiki_title': 'Vive Latino', 'wiki_page_id': 83800},\n",
       " {'wiki_title': 'Gatos fantasma', 'wiki_page_id': 6969933}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Donde vive Bill Gates'\n",
    "get_wiki_search(query, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_article(page_id, lang_code='es', min_sentence_length=80):\n",
    "    url = 'https://%s.wikipedia.org/w/api.php' % lang_code + \\\n",
    "        '?action=parse' + \\\n",
    "        '&pageid=%d' % page_id + \\\n",
    "        '&prop=text' + \\\n",
    "        '&format=json'\n",
    "        #'&section=1' + \\\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    if r.status_code != 200:\n",
    "        print('')\n",
    "        print('Unexpected HTTP Status: %d - %s' % (r.status_code, url))\n",
    "        #raise Exception('Unexpected HTTP Status: %d' % r.status_code)\n",
    "        return None\n",
    "    \n",
    "    json_data = r.json()\n",
    "    if 'parse' not in json_data:\n",
    "        # Not found\n",
    "        return None\n",
    "    article = json_data['parse']['text']['*']\n",
    "    article = re.sub(r'><', \">\\n<\", article) # Adds new line between HTML tags\n",
    "    \n",
    "    soup = BeautifulSoup(article)\n",
    "    article = soup.get_text().strip()\n",
    "    \n",
    "    # Remove URLs\n",
    "    url_regex = r'(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))'\n",
    "    article = re.sub(url_regex, ' ', article)\n",
    "    \n",
    "    # Clean short sentences\n",
    "    article = re.sub(r'\\s*\\n\\s*', '\\n', article)\n",
    "    article = article.splitlines()\n",
    "    article = [x for x in article if len(x) > min_sentence_length]\n",
    "    article = '\\n'.join(article) + ''\n",
    "    \n",
    "    # Clean wiki rubbish\n",
    "    article = re.sub(r'\\[[^\\]]+\\]+', ' ', article)\n",
    "    article = re.sub(r'[\\s\\u200b]+', ' ', article)\n",
    "    \n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'California es uno de los cincuenta estados que, junto con Washington D. C., forman los Estados Unidos de América. Su capital es Sacramento, y su ciudad más poblada, Los Ángeles. Está ubicado en la región oeste del país, división Pacífico, limitando al norte con Oregón, al este con Nevada, al sureste con el río Colorado que lo separa de Arizona, al sur con Baja California (México) y al oeste con el océano Pacífico. Con 37 253 956 habitantes en 2010 es el estado más poblado y con 423 970 km², el tercero más extenso, por detrás de Alaska y Texas. Fue admitido en la Unión el 9 de septiembre de 1850 como el estado número 31. Además, cuenta con las segunda y quinta áreas más pobladas de la nación, el Gran Los Ángeles y el Área de la Bahía de San Francisco y ocho de las ciudades más pobladas del país: Los Ángeles, San Diego, San José, San Francisco, Fresno, Sacramento, Long Beach y Oakland. La zona estuvo poblada desde hace milenios por los nativos americanos antes de las primeras expediciones europeas en el siglo XVI. Estos pobladores se repartían en 105 pueblos indígenas americanos que hablaban los idiomas de seis familias lingüísticas diferentes. La Corona española colonizó las áreas de la costa del territorio en 1769 antes de que este pasara a formar parte de México tras la Guerra de la Independencia de México (1810-1821). California fue parte del territorio mexicano hasta la guerra entre México y los Estados Unidos de 1846-1848. Al término de la guerra y como condición para la paz, la República Mexicana fue obligada a ceder el territorio a los Estados Unidos en el Tratado de Guadalupe Hidalgo. La en el período 1848-1849 provocó una inmigración de 90 000 estadounidenses procedentes del resto del país. Finalmente, California se convirtió en el trigésimo primer Estado de Estados Unidos en 1850. Si California fuera una nación independiente, sería la quinta economía del mundo, con un producto interior bruto (PIB) de alrededor de 2.80 billones de dólares (datos de 2019) lo que representa el 12,4 % del PIB de Estados Unidos, que asciende a un total de 19,3 billones de dólares. Las principales actividades económicas del estado son la agricultura, el ocio, la energía eléctrica y el turismo. En California se localizan algunas de las ciudades económicas más importantes del mundo, tales como Los Ángeles (entretenimiento, ocio), el Valle Central (agricultura), Originalmente la palabra California se refería a una región más amplia, compuesta por el territorio del actual estado de California más la totalidad o parte de Nevada, Utah, Arizona y Wyoming y la mexicana península de California. La teoría más respaldada es que la palabra California deriva del nombre de la regente de un paraíso ficticio dominado por amazonas negras, la reina Calafia. El mito de Calafia se registra en una obra de 1510, El reino de Calafia o Califia es descrito por Montalvo como una tierra remota habitada por grifos y otras extrañas bestias y rico en oro: Es conocido que a mano derecha de las Indias hay una isla llamada California, muy cerca de esa parte del paraíso terrenal, que está habitada por mujeres negras, sin un solo hombre entre ellas, que viven al estilo de las amazonas. Tenían el cuerpo robusto, con corazones fuertes y apasionados y grandes virtudes. La isla misma es una de las más salvajes del mundo por sus escarpadas y llamativas rocas. Sus armas están todas hechas de oro. La isla está repleta de oro y piedras preciosas por todas partes, hasta el punto que no hay otros metales. California es el quinto nombre de origen europeo más antiguo en Estados Unidos. Fue impuesto en la expedición española dirigida por Diego de Becerra y Fortún Jiménez, que denominaron al extremo inferior de la península de California cuando desembarcaron allí en 1533 por mandato de Hernán Cortés. El texto que sigue es una traducción defectuosa. Si quieres colaborar con Wikipedia, busca el artículo original y mejora esta traducción. Copia y pega el siguiente código en la página de discusión del autor de este artículo: {{subst:Aviso mal traducido|California}} ~~~~ La historia de California comenzó con los nativos americanos que llegaron primero a California hace unos 13.000-15.000 años atrás. La exploración y asentamiento de los europeos a lo largo de las costas y de los valles del interior comenzó en el siglo XVI con la llegada de los españoles. California fue adquirida por los Estados Unidos bajo los términos del Tratado de Guadalupe Hidalgo en 1848 tras la derrota de México en la Intervención estadounidense en México, causando la expansión hacia el oeste por parte de los Estados Unidos en México. Con la intensificación de la Fiebre de oro de California, en la década de 1850, California se sumó a la Unión como un estado en 1850. A finales del siglo XIX. California seguía siendo en gran parte rural y agrícola, pero tenía una población de alrededor de 1,4 millones de europeos. California es el tercer estado más grande de Estados Unidos por área, después de Alaska y Texas. California a menudo se divide geográficamente en dos regiones, el sur de California, que comprende los 10 condados más al sur, y el norte de California, que comprende los 48 condados más al norte. Limita con Oregón al norte, Nevada al este y noreste, Arizona al sureste, el Océano Pacífico al oeste y comparte una frontera internacional con el estado mexicano de Baja California al sur (con el que forma parte de la región californiana de América del Norte, junto con Baja California Sur). En el centro del estado se encuentra el Valle Central de California, rodeado por la Sierra Nevada en el este, las cordilleras costeras en el oeste, la Cordillera de las Cascadas al norte y por las montañas de Tehachapi en el sur. El Valle Central es el corazón agrícola productivo de California. La Sierra Nevada abarca el Valle de Yosemite, famoso por sus rocas talladas por la erosión glaciar, y el parque nacional Sequoia, hogar de las gigantescas secuoyas, los organismos vivos más grandes de la Tierra, y el Lago Tahoe, el lago más grande del estado por volumen. El estado de California es un territorio muy diverso dividido a grandes rasgos por la cordillera de Sierra Nevada, la costa y un gran valle central. En California se encuentra el punto más alto (monte Whitney) y el más bajo (Valle de la Muerte) de los 48 estados contiguos. El estado está dividido entre el Norte de California y el Sur de California, aunque la frontera entre ambas regiones no está muy bien definida. San Francisco es considerada como una ciudad del Norte de California y Los Ángeles como una ciudad del Sur de California, pero algunas zonas entre ambas no comparten esa misma identidad. El servicio Geológico de los Estados Unidos define al centro geográfico del estado en el punto cerca de North Fork. Los geógrafos suelen dividir el estado en once provincias geomorfológicas con límites claramente definidos. Son, de norte a sur, las montañas Klamath, la cordillera de las Cascadas, la Placa Modoc, las cuencas y cordilleras, la cadena costera del Pacífico, el Valle Central, Sierra Nevada, las cordilleras Transversales, el desierto de Mojave, las cordilleras Peninsulares, y el desierto de Colorado. Para propósitos de explicación, también es útil reconocer a la cuenca Los Ángeles, el archipiélago del Norte, y el océano Pacífico. Aunque la mayor parte del estado tiene un clima mediterráneo, debido al gran tamaño del estado, el clima varía desde árido a subártico, dependiendo de la latitud, elevación, y proximidad a la costa. La fresca corriente de California en alta mar a menudo crea una niebla de verano cerca de la costa. Más al interior, hay inviernos más fríos y veranos más calurosos. La moderación marítima hace que las temperaturas de verano en la costa de Los Ángeles y San Francisco sean las más suaves de todas las principales áreas metropolitanas de los Estados Unidos y que sean especialmente frescas en comparación con las áreas en la misma latitud en el interior. Las partes del norte del estado reciben más lluvia que el sur. Las cadenas montañosas de California también influyen en el clima: algunas de las partes más lluviosas del estado son las laderas montañosas orientadas al oeste. El noroeste de California tiene un clima templado, y el Valle Central tiene un clima mediterráneo pero con temperaturas más altas que la costa. Las montañas altas, incluida la Sierra Nevada, tienen un clima alpino con notables nevadas en invierno y un calor leve a moderado en verano. El apodo del estado es The Golden State (El Estado Dorado), nombre que algunos suponen quizás provenga de los numerosos días en los que brilla el sol durante el año, o quizás del color dorado que pueden tomar los montes a ciertas horas del día (tal como ocurre en muchas otras partes del mundo), aunque la probabilidad más cierta de tal apodo (del mismo modo que la del apodo dado a la boca de la bahía de San Francisco: Golden Gate = Puerta Dorada) remite al periodo de la fiebre del oro. El estado de California, en los Estados Unidos, está dividido en 58 condados. Los condados tienen sus propias elecciones, recaudación de impuestos sobre la propiedad, mantenimiento de registros tales como escrituras de propiedad y tribunales locales dentro de su territorio, también son los encargados del cumplimiento de la ley (a través de los de cada condado y de sus asistentes) en áreas que no están incorporadas en ciudades. La Oficina del Censo de los Estados Unidos estima que la población de California fue de 38 041 430 habitantes al 1 de julio de 2012, un 2,1 % de incremento desde el censo de 2010. Entre 2000 y 2009, hubo un incremento natural de 3 090 016 (5 058 440 nacimientos menos 2 179 958 muertes). Durante este período, la migración internacional produjo un aumento de 1 816 633 personas, mientras que la migración interna produjo una disminución de 1 509 708, lo que resulta en una inmigración neta de 306 925 personas. Las estadísticas del Estado de California muestran una población de 38 292 687 al 1 de enero de 2009. Sin embargo, de acuerdo con el Manhattan Institute for Policy Research, desde 1990 casi 3,4 millones de californianos se han mudado a otros estados. California es la segunda entidad subnacional más poblada del hemisferio occidental y del continente americano, tras el estado de São Paulo, en Brasil. Además, el condado de Los Ángeles ha celebrado el título de condado más poblado de Estados Unidos durante décadas, es más populoso que 42 de los estados estadounidenses. California es el hogar de ocho de las 50 ciudades más pobladas en los Estados Unidos: Los Ángeles, San Diego, San José, San Francisco, Fresno, Sacramento, Long Beach, y Oakland. El centro de población del estado está localizado en el pueblo de Buttonwillow, condado de Kern. La lengua oficial es el inglés, hablado en el hogar por 60,5 % de la población californiana. El español es la segunda lengua en número de hablantes, con un 35,8 % de la población. La sección 1632 del Código Civil de California reconoce el idioma español, de ahí que la ley Dymally-Alatorre sobre servicios bilingües, instituya un bilingüismo inglés-español, sin la exclusión necesaria de otras lenguas. El resto de idiomas, como chino, tagalo, vietnamita, llegan hasta el 6,08 % del total. Los idiomas indígenas del estado, que apenas suponen unas decenas de miles de hablantes, pertenecientes sobre todo a los grupos hokano y penutio, son lenguas amenazadas: muchas de ellas son habladas solo por las generaciones más ancianas que son bilingües, mientras que muchos niños amerindios son monolingües en inglés. La mayor parte de población de California se concentra en tres grandes áreas metropolitanas: Área de la Bahía de San Francisco: San Francisco, San José, Oakland, Fremont, Santa Rosa... Otras ciudades de importancia son Sacramento (capital del Estado), Fresno, Bakersfield, Riverside, Stockton, Modesto... California es tradicionalmente una gran potencia económica, pionera y líder en numerosos segmentos de la industria como la aeronáutica, la técnica espacial, la informática, la electrónica, la industria médica, etc. Por ello, California sería por sí misma una de las siete potencias mundiales. También tiene una agricultura muy desarrollada, favorecida por su clima benigno. California dispone de extensos cultivos de cítricos. En los últimos decenios ha desarrollado también la producción de vino (particularmente en el Valle Napa). Posee importantes actividades mineras como la dedicada al oro. Silicon Valley, sede de muchas de las empresas tecnológicas más importantes del mundo. Los Angeles Memorial Coliseum, el único estadio del mundo que ha albergado la celebración de dos Juegos Olímpicos de Verano: en 1932 y en 1984. California fue sede de los Juegos Olímpicos de Squaw Valley 1960, de los Juegos Olímpicos de Los Ángeles 1932 y Los Ángeles 1984, así como de la Copa Mundial de Fútbol de 1994. Actualmente se prepara para los Juegos Olímpicos de Los Ángeles 2028. California tiene veintiún franquicias en las Grandes Ligas de deportes profesionales, muchas más que cualquier otro estado. El Área de la Bahía de San Francisco tiene siete equipos en las grandes ligas en tres ciudades, San Francisco, Oakland y San José. Mientras el Área Metropolitana de Los Ángeles es sede de once franquicias en las Grandes Ligas profesionales. San Diego tiene dos equipos de liga principales, y Sacramento también tiene dos. Sede de algunas de las más prominentes universidades de los Estados Unidos, California tiene desde hace mucho tiempo respetados programas de deportes universitarios. En particular, los programas atléticos de UC Berkeley, USC, UCLA, Stanford y Fresno State a menudo se clasifican a nivel nacional en varios deportes universitarios. California también es sede del más antiguo de los títulos universitarios denominados \"bowl\", el anual Rose Bowl, y el Holiday Bowl, entre otros. Numerosos circuitos de carreras de Estados Unidos se hallan en California. Los principales son el óvalo de Fontana, el callejero de Long Beach y los autódromos de Laguna Seca y Sears Point, así como los desaparecidos Riverside y Ontario. Además de albergar los principales certámenes estadounidenses de automovilismo y motociclismo, el Gran Premio de Long Beach fue una prueba válida para el Campeonato Mundial de Fórmula 1 y actualmente recibe a la IndyCar Series y el United SportsCar Championship, en tanto que Laguna Seca ha albergado pruebas del Campeonato Mundial de Motociclismo y el Campeonato Mundial de Superbikes. En California se realizan numerosos torneos de golf, entre ellos el Abierto de Los Ángeles y Abierto de San Diego. Los campos de golf de Pebble Beach y Olympic han sido sede de varias ediciones del Abierto de los Estados Unidos. En tenis, el Masters de Indian Wells pertenece al ATP World Tour Masters 1000 y WTA Premier, y el Torneo de Stanford al WTA Premier. Anteriormente se realizaron el Torneo de Los Ángeles y el Torneo de San José. En polo se efectuó el Campeonato Mundial de Polo de 1998 en Santa Bárbara y en 2022 se efectuará la XII versión del mundial en Indio. A continuación se muestra una lista de los principales equipos de las grandes ligas de deporte profesional de California: La California del siglo XIX está retratada en los Cuentos californianos (1922) de Adolfo Carrillo (1855-1926). La vida en la California del Siglo XIX aparece reflejada en la película \"La máscara del Zorro\", dirigida por Martin Campbell y protagonizada por Antonio Banderas, Catherine Zeta Jones y Anthony Hopkins, así como en su secuela \"La leyenda del Zorro\", cuyo argumento transcurre durante la incorporación de este territorio a los Estados Unidos. «Per capita personal income». Archivado desde el original el 6 de agosto de 2010. Consultado el 4 de septiembre de 2010. «Copia archivada». Archivado desde el original el 17 de julio de 2011. Consultado el 10 de julio de 2011. «Government Code Section 420-429.8» (en inglés). Official California Legislative Information. Archivado desde el original el 28 de junio de 2009. Consultado el 11 de septiembre de 2009. Gudde, Erwin G. and William Bright. 2004. California Place Names: The Origin and Etymology of Current Geographical Names, pp.59-60 «Online Etymology Dictionary». . 24 de junio de 1957. Consultado el 2 de julio de 2010. Stewart, George (1945). Names on the Land: A Historical Account of Place-Naming in the United States. New York: Random House. pp. 11–17. «Table 1. Annual Estimates of the Population for the United States, Regions, States, and Puerto Rico: April 1, 2010 to July 1, 2012» (CSV). 2012 Population Estimates. United States Census Bureau, Population Division. diciembre de 2012. Archivado desde el original el 29 de diciembre de 2012. Consultado el 22 de diciembre de 2012. «Table 4. Cumulative Estimates of the Components of Resident Population Change for the United States, Regions, States, and Puerto Rico: April 1, 2000 to July 1, 2009» (CSV). . 22 de diciembre de 2009. Archivado desde el original el 9 de junio de 2010. Consultado el 26 de.diciembre de 2009. Gray, Tom; Scardamalia, Robert (septiembre de 2012). «The Great California Exodus: A Closer Look». Manhattan Institute for Policy Research, Inc «About Los Angeles County Department of Public Social Services». Los Angeles County Department of Public Social Services. diciembre de 2005. Archivado desde el original el 17 de abril de 2010. Consultado el 26 de diciembre de 2009. Barrett, Beth (19 de septiembre de 2003). «Baby Slump In L.A. County». Los Angeles Daily News (Los Angeles Newspaper Group). pp. N4. Consultado el 26 de diciembre de 2009. «Population and Population Centers by State: 2000» (TXT). United States Census 2000. US Census Bureau Geography Division. 20 de mayo de 2002. Archivado desde el original el 23 de febrero de 2010. Consultado el 26 de diciembre de 2009. . Alicante: Biblioteca Virtual Miguel de Cervantes. Consultado el 1 de octubre de 2018. Portillo y Díaz de Solano, Álvaro del. (1982) Descubrimientos y exploraciones en las costas de California (1532-1650).'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = get_wiki_article(12918)\n",
    "article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static entities\n",
    "\n",
    "Sometimes, questions or answers contain proper nouns (with an initial capital letter) or quoted text. Those elements are considered \"static entities\" in this notebook and we can extract them using this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_static_entities(text, default_score=5.):\n",
    "    all_matches = []\n",
    "    matches = re.findall(r'\"(.+?)\"', text)\n",
    "    if len(matches) > 0:\n",
    "        all_matches += [x.strip() for x in matches]\n",
    "    matches = re.findall(r'\\'(.+?)\\'', text)\n",
    "    if len(matches) > 0:\n",
    "        all_matches += [x.strip() for x in matches]\n",
    "    matches = re.findall(r'((?:(?:[A-Z][A-Za-z]+|de)\\s?)+)', text)\n",
    "    if len(matches) > 0:\n",
    "        all_matches += [x.strip() for x in matches]\n",
    "    \n",
    "    # Format static entities\n",
    "    static_entities = []\n",
    "    for x in all_matches:\n",
    "        static_entities.append({\n",
    "            'entity_text': x,\n",
    "            'entity_score': default_score,\n",
    "            'is_mandatory': True,\n",
    "        })\n",
    "    \n",
    "    return static_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_tokenizer(artifacts_path='../artifacts/', lm_name='bert-base-multilingual-cased', lowercase=False):\n",
    "    save_path = '%s%s/' % (artifacts_path, lm_name)\n",
    "    tokenizer = BertTokenizerFast('%svocab.txt' % save_path, do_lower_case=lowercase)\n",
    "    return lambda text : [text[start:end] \\\n",
    "                          for (start, end) in tokenizer(text, return_offsets_mapping=True,\n",
    "                                                        return_special_tokens_mask=False)['offset_mapping'][1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def bleu_score(reference, hypothesis):\n",
    "    if len(reference) == 1:\n",
    "        return int(reference == hypothesis)\n",
    "    elif len(reference) < 4:\n",
    "        weights_len = [(0.5, 0.5), (0.34, 0.33, 0.33)]\n",
    "        score = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis,\n",
    "                                                        weights=weights_len[len(reference)-1])\n",
    "    else:\n",
    "        score = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
    "    return score\n",
    "\"\"\"\n",
    "\n",
    "def bleu_score(reference, hypothesis):\n",
    "    matches = [int(x == y) for x, y in zip(reference, hypothesis)]\n",
    "    return sum(matches) / len(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['esta', 'es', 'prueba','1']\n",
    "b = ['esta', 'es', 'prueba','2']\n",
    "\n",
    "bleu_score(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['¡', 'Ho', 'la', 'mundo', '!', '¡', 'Adi', 'ós', 'mundo', '!']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '¡Hola mundo! ¡Adiós mundo!'\n",
    "\n",
    "tokenizer = get_word_tokenizer()\n",
    "tokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MKQA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_es_number_unit(raw_unit_name, is_plural=False):\n",
    "    # First list correspond to singular and second list to plural\n",
    "    es_conversion = {\n",
    "        'Antes de la era vulgar': [['a.C.'], ['a.C.']],\n",
    "        'Galones': [['galón'], ['galones']],\n",
    "        'Millas por hora': [['mph', 'milla por hora'], ['mph', 'millas por hora']],\n",
    "        'acre': [['acre'], ['acres']],\n",
    "        'antes del Mediodia': [['AM', 'A.M.'], ['AM', 'A.M.']],\n",
    "        'año terrestre': [['año'], ['años']],\n",
    "        'caballo de potencia metrico': [['caballo de potencia', 'caballo', 'hp', 'cv'], ['caballos de potencia', 'caballos', 'hp', 'cv']],\n",
    "        'centímetro': [['cm', 'centímetro'], ['cm', 'centímetros']],\n",
    "        'día': [['día'], ['días']],\n",
    "        'dólar estadounidense': [['dólar', '$'], ['dólares', '$']],\n",
    "        'episodio': [['episodio'], ['episodios']],\n",
    "        'escala Fahrenheit': [['grado Fahrenheit', 'Fahrenheit', 'ºF'], ['grados Fahrenheit', 'Fahrenheits', 'ºF']],\n",
    "        'estaciones del año': [['temporada'], ['temporadas']],\n",
    "        'grados centigrados': [['grado centigrado', 'grado', 'ºC'], ['grados centigrados', 'grados', 'ºC']],\n",
    "        'gramo': [['gramo', 'gr', 'g'], ['gramos', 'gr', 'g']],\n",
    "        'hora': [['hora', 'h'], ['horas', 'h']],\n",
    "        'kilometraje': [['kilómetro', 'km'], ['kilómetros', 'km']],\n",
    "        'libra avoirdupois': [['libra avoirdupois', 'libra', 'lb'], ['libras avoirdupois', 'libras', 'lb']],\n",
    "        'light año terrestre': [['año luz'], ['años luz']],\n",
    "        'mes sinódico': [['mes sinódico', 'mes'], ['meses sinódicos', 'meses']],\n",
    "        'metros': [['metro', 'm'], ['metros', 'm']],\n",
    "        'metros por segundo': [['metro por segundo', 'mps', 'm/s'], ['metros por segundo', 'mps', 'm/s']],\n",
    "        'mililitro': [['mililitro', 'ml'], ['mililitros', 'ml']],\n",
    "        'milimetro': [['milímetro', 'mm'], ['milímetros', 'mm']],\n",
    "        'milla': [['milla', 'mi'], ['millas', 'mi']],\n",
    "        'millas cuadradas': [['milla cuadrada'], ['millas cuadradas']],\n",
    "        'minuto': [['minuto'], ['minutos']],\n",
    "        'onza': [['onza'], ['onzas']],\n",
    "        'other currency': [[], []], # Nothing to do\n",
    "        'other unit': [[], []], # Nothing to do\n",
    "        'palabra': [['palabra'], ['palabras']],\n",
    "        'pie': [['pie'], ['pies']],\n",
    "        'pies cuadrados': [['pie cuadrado'], ['pies cuadrados']],\n",
    "        'post meridiem (time)': [['PM', 'P.M.'], ['PM', 'P.M.']],\n",
    "        'pulgada': [['pulgada'], ['pulgadas']],\n",
    "        'segundos': [['segundo'], ['segundos']],\n",
    "        'septenario': [['septenario'], ['septenarios']],\n",
    "        'tanto por ciento': [['porcentaje', '%'], ['porcentaje', '%']],\n",
    "    }\n",
    "    \n",
    "    if raw_unit_name not in es_conversion or len(es_conversion[raw_unit_name]) == 0:\n",
    "        return []\n",
    "    else:\n",
    "        return [x for x in es_conversion[raw_unit_name][int(is_plural)]]\n",
    "\n",
    "def parse_nwu_answer_es(raw_answer):\n",
    "    \"\"\"\n",
    "    Parses Spanish answers of type number_with_unit (nwu).\n",
    "    \"\"\"\n",
    "    answers = []\n",
    "\n",
    "    x_interval = re.search(r'^([\\d.]+) ([\\d.]+) (.+)$', raw_answer)\n",
    "    if x_interval:\n",
    "        unit_value_1 = str_to_num(x_interval[1])\n",
    "        unit_value_2 = str_to_num(x_interval[2])\n",
    "        unit_names = get_es_number_unit(x_interval[3], is_plural=True)\n",
    "        if len(unit_names) == 0:\n",
    "            # Skip if not unit name is provided in range of values\n",
    "            pass\n",
    "        else:\n",
    "            for unit_name in unit_names:\n",
    "                answers.append('entre %s y %s %s' % (str(unit_value_1), str(unit_value_2), unit_name))\n",
    "                answers.append('desde %s hasta %s %s' % (str(unit_value_1), str(unit_value_2), unit_name))\n",
    "    else:\n",
    "        x_single = re.search(r'^([\\d.]+) (.+)$', raw_answer)\n",
    "        if x_single is None:\n",
    "            return []\n",
    "        unit_value = str_to_num(x_single[1])\n",
    "        unit_names = get_es_number_unit(x_single[2], is_plural=(unit_value > 0))\n",
    "        if len(unit_names) == 0:\n",
    "            answers.append(str(unit_value))\n",
    "        else:\n",
    "            for unit_name in unit_names:\n",
    "                answers.append('%s %s' % (str(unit_value), unit_name))\n",
    "    \n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date_answer_es(raw_answer):\n",
    "    date_parts = re.search('^([\\d.]+)-([\\d.]+)-([\\d.]+)$', raw_answer)\n",
    "    if date_parts is None:\n",
    "        # No ISO date format\n",
    "        return [raw_answer]\n",
    "    \n",
    "    month_conversion = ['enero', 'febrero', 'marzo', 'abril', 'mayo', 'junio', 'julio',\n",
    "                        'agosto', 'septiembre', 'octubre', 'noviembre', 'diciembre']\n",
    "    \n",
    "    year_number = date_parts[1]\n",
    "    month_number = date_parts[2]\n",
    "    month_str = month_conversion[str_to_num(date_parts[2]) - 1]\n",
    "    day_number = date_parts[3]\n",
    "    \n",
    "    return [\n",
    "        '%s-%s-%s' % (year_number, month_number, day_number),\n",
    "        '%s-%s-%s' % (day_number, month_number, year_number),\n",
    "        '%s-%s-%s' % (month_number, day_number, year_number),\n",
    "        '%s/%s/%s' % (month_number, day_number, year_number),\n",
    "        '%s de %s del %s' % (day_number, month_number, year_number),\n",
    "        '%s de %s, %s' % (day_number, month_number, year_number),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_raw_answer_es(type, main_answer, aliases):\n",
    "    \"\"\"\n",
    "    Parses MKQA Answers of Spanish language.\n",
    "    \"\"\"\n",
    "    parsed_answers = []\n",
    "    if type == 'number_with_unit':\n",
    "        # Example: 16.0 año terrestre\n",
    "        parsed_answers += parse_nwu_answer_es(main_answer)\n",
    "        for alias in aliases:\n",
    "            parsed_answers += parse_nwu_answer_es(alias)\n",
    "    elif type == 'number':\n",
    "        # Example: 104.0\n",
    "        parsed_answers.append(main_answer)\n",
    "        for alias in aliases:\n",
    "            parsed_answers.append(alias)\n",
    "    elif type == 'date':\n",
    "        # Example: 2001-08-29\n",
    "        parsed_answers += parse_date_answer_es(main_answer)\n",
    "    elif type == 'entity':\n",
    "        # Example: Pokémon Ranger: Sombras de Almia\n",
    "        parsed_answers.append(main_answer.strip())\n",
    "        for alias in aliases:\n",
    "            parsed_answers.append(alias.strip())\n",
    "    elif type == 'short_phrase':\n",
    "        # Example: rosemary almond\n",
    "        parsed_answers.append(main_answer.strip())\n",
    "        for alias in aliases:\n",
    "            parsed_answers.append(alias.strip())\n",
    "    else:\n",
    "        # Ignored types: unanswerable, long_answer, binary\n",
    "        pass\n",
    "    \n",
    "    return parsed_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_answers(context, answers, word_tokenizer, min_bleu):\n",
    "    context_tokens = word_tokenizer(context)\n",
    "    found_answers = []\n",
    "    \n",
    "    context_offsets = []\n",
    "    last_span = 0\n",
    "    for token in context_tokens:\n",
    "        span_start = context.find(token, last_span)\n",
    "        if span_start == -1:\n",
    "            raise Exception('Error to tokenize context: %s' % context)\n",
    "        span_end = span_start + len(token)\n",
    "        context_offsets.append([span_start, span_end])\n",
    "    \n",
    "    for answer in answers:\n",
    "        answer_tokens = word_tokenizer(answer)\n",
    "        window_size = len(answer_tokens)\n",
    "        \n",
    "        i = 0\n",
    "        len_answer = len(answer_tokens)\n",
    "        while i < len(context_tokens) - len_answer:\n",
    "            score = bleu_score(answer_tokens, context_tokens[i:i+window_size])\n",
    "            if score >= min_bleu:\n",
    "                span_start = context_offsets[i][0]\n",
    "                span_end = context_offsets[i+window_size-1][1]\n",
    "                found_answers.append({\n",
    "                    'answer_start': span_start,\n",
    "                    'answer_end': span_end,\n",
    "                    'text': context[span_start:span_end],\n",
    "                })\n",
    "                i += len_answer # Skips answer position\n",
    "            else:\n",
    "                i += 1\n",
    "    \n",
    "    return found_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_entities(context, entities, n=1):\n",
    "    if entities is None or len(entities) == 0:\n",
    "        return True, 0.\n",
    "    \n",
    "    counter = 0\n",
    "    max_score = 0.\n",
    "    current_score = 0.\n",
    "    for entity in entities:\n",
    "        max_score = entity['entity_score']\n",
    "        if context.find(entity['entity_text']) != -1:\n",
    "            # Found entity\n",
    "            counter += 1\n",
    "            current_score += entity['entity_score']\n",
    "        elif 'is_mandatory' in entity and entity['is_mandatory']:\n",
    "            # Mandatory entitiy not found\n",
    "            return False, 0.\n",
    "    \n",
    "    return (counter >= n), (current_score / max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_tokens(context, tokens, token_score=1.):\n",
    "    if tokens is None or len(tokens) == 0:\n",
    "        raise Exception('No tokens provided for context: %s' % context)\n",
    "    \n",
    "    max_score = 0\n",
    "    current_score = 0.\n",
    "    for token in tokens:\n",
    "        max_score += token_score\n",
    "        if context.find(token) != -1:\n",
    "            # Found token\n",
    "            current_score += token_score\n",
    "    \n",
    "    return current_score / max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_qa_entities(entities_a, entities_b):\n",
    "    entities = {}\n",
    "    for entity in entities_a:\n",
    "        idx = entity['wiki_page_id'] if 'wiki_page_id' in entity else entity['entity_text']\n",
    "        if idx in entities:\n",
    "            if entity['entity_score'] < entities[idx]['entity_score']:\n",
    "                entities[idx] = entity\n",
    "        else:\n",
    "            entities[idx] = entity\n",
    "    for entity in entities_b:\n",
    "        idx = entity['wiki_page_id'] if 'wiki_page_id' in entity else entity['entity_text']\n",
    "        if idx in entities:\n",
    "            if entity['entity_score'] < entities[idx]['entity_score']:\n",
    "                entities[idx] = entity\n",
    "        else:\n",
    "            entities[idx] = entity\n",
    "    return list(entities.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_squad_items(squad_items, top_items):\n",
    "    squad_items = sorted(squad_items, key=lambda kv: kv['score'], reverse=True)\n",
    "    return squad_items[:top_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_squad_item(idx, question, answers, answer_types, word_tokenizer, query_top_results=3,\n",
    "                    entity_top_results=3, min_bleu=0.8, min_entity_score=0.96, lang_code='es',\n",
    "                    max_tokens_length=512, max_chars_length=1500, verbose=None):\n",
    "    question_tokens = word_tokenizer(question)\n",
    "    squad_items = []\n",
    "    \n",
    "    # Get top results of question and answers (full text search)\n",
    "    question_page_ids = get_wiki_search(question, top_results=query_top_results, lang_code=lang_code)\n",
    "    answer_page_ids = []\n",
    "    for answer, answer_type in zip(answers, answer_types):\n",
    "        if answer_type == 'entity':\n",
    "            # Only search wiki pages using the answer if it is an entity\n",
    "            answer_page_ids += get_wiki_search(answer, top_results=query_top_results, lang_code=lang_code)\n",
    "    \n",
    "    # Get top results of static entities in question and answers\n",
    "    q_static_entities = get_static_entities(question)\n",
    "    for entity in q_static_entities:\n",
    "        question_page_ids += get_wiki_search(entity['entity_text'], top_results=query_top_results, lang_code=lang_code)\n",
    "    a_static_entities = []\n",
    "    for answer in answers:\n",
    "        a_static_entities += get_static_entities(answer)\n",
    "    for entity in a_static_entities:\n",
    "        answer_page_ids += get_wiki_search(entity['entity_text'], top_results=query_top_results, lang_code=lang_code)\n",
    "    \n",
    "    # Get top results of dbpedia entities in quesiton\n",
    "    question_entities = get_dbpedia_search(question, min_score=min_entity_score)\n",
    "    answers_entities = []\n",
    "    #for answer, answer_type in zip(answers, answer_types):\n",
    "    #    if answer_type in ['number', 'number_with_unit']:\n",
    "    #        continue\n",
    "    #    answers_entities += get_dbpedia_search(answer, min_score=min_entity_score)\n",
    "    \n",
    "    question_entities = combine_qa_entities(question_entities, q_static_entities)\n",
    "    answers_entities = combine_qa_entities(answers_entities, a_static_entities)\n",
    "    found_entities = combine_qa_entities(question_entities, answers_entities)\n",
    "\n",
    "    # Given a set of entities, create a common dictionary of Wiki Page IDs from entities and OpenSearch results\n",
    "    entity_page_ids = {}\n",
    "    for entity in found_entities:\n",
    "        if 'wiki_page_id' not in entity:\n",
    "            continue\n",
    "        elif entity['wiki_page_id'] in entity_page_ids:\n",
    "            if entity_page_ids[entity['wiki_page_id']]['entity_score'] < entity['entity_score']:\n",
    "                entity_page_ids[entity['wiki_page_id']] = entity\n",
    "        else:\n",
    "            entity_page_ids[entity['wiki_page_id']] = entity\n",
    "    \n",
    "    entity_page_ids = sorted(entity_page_ids.items(), key=lambda kv: kv[1]['entity_score'], reverse=True)\n",
    "    entity_page_ids = [x[1] for x in entity_page_ids][:entity_top_results]\n",
    "    \n",
    "    all_page_ids = {x['wiki_page_id']:x for x in question_page_ids}\n",
    "    for answer_page_data in answer_page_ids:\n",
    "        if answer_page_data['wiki_page_id'] not in all_page_ids:\n",
    "            all_page_ids[answer_page_data['wiki_page_id']] = answer_page_data\n",
    "    for entity_page_data in entity_page_ids:\n",
    "        if entity_page_data['wiki_page_id'] not in all_page_ids:\n",
    "            all_page_ids[entity_page_data['wiki_page_id']] = entity_page_data\n",
    "    all_page_ids = list(all_page_ids.values())\n",
    "    \n",
    "    # Get page content of each ID\n",
    "    for i, page_data in enumerate(all_page_ids):\n",
    "        if verbose:\n",
    "            print('- Item: %d / %d | Page: %d / %d | Found: %d' % (verbose['i'], verbose['n_items'], i+1,\n",
    "                    len(all_page_ids), verbose['n_found']), ' '*20, end='\\r')\n",
    "    \n",
    "        page_id = page_data['wiki_page_id']\n",
    "        page_title = page_data['wiki_title']\n",
    "        page_content = get_wiki_article(page_id, lang_code=lang_code)\n",
    "        \n",
    "        if page_content is None:\n",
    "            continue\n",
    "        \n",
    "        # Split content into sentences\n",
    "        content_sentences = nltk.tokenize.sent_tokenize(page_content)\n",
    "        \n",
    "        group_sentences = ''\n",
    "        k = 0\n",
    "        while k < len(content_sentences):\n",
    "            sentence = content_sentences[k]\n",
    "            tmp_group_sentences = (group_sentences + ' ' + sentence).strip()\n",
    "            tmp_group_tokens = word_tokenizer(tmp_group_sentences)\n",
    "            \n",
    "            tokens_length = len(tmp_group_tokens) + len(question_tokens) # Context + question\n",
    "            chars_length = sum(len(token) for token in tmp_group_tokens) # Only context\n",
    "            if tokens_length >= (max_tokens_length - 3) and chars_length >= max_chars_length:\n",
    "                # Context must have at least N entities of the question and M entities of the answer\n",
    "                has_question_entities, question_entities_score = has_entities(group_sentences, question_entities, n=1)\n",
    "                has_answer_entities, answer_entities_score = has_entities(group_sentences, answers_entities, n=1)\n",
    "                if has_question_entities and has_answer_entities:\n",
    "                    # Note that we substract 3 since we add 3 additional tokens when encoding for QA model training\n",
    "                    # Try to find answers in the grouped sentences\n",
    "                    found_answers = find_answers(group_sentences, answers, word_tokenizer, min_bleu=min_bleu)\n",
    "                    tokens_score = has_tokens(group_sentences, question_tokens)\n",
    "                    if len(found_answers) > 0:\n",
    "                        squad_item = {\n",
    "                            'score': question_entities_score + answer_entities_score,\n",
    "                            'title': page_title,\n",
    "                            'paragraphs': [{\n",
    "                                'context': group_sentences,\n",
    "                                'qas': [{\n",
    "                                    'id': idx,\n",
    "                                    'question': question,\n",
    "                                    'answers': [],\n",
    "                                }],\n",
    "                            }],\n",
    "                        }\n",
    "                        for found_answer in found_answers:\n",
    "                            squad_item['paragraphs'][0]['qas'][0]['answers'].append({\n",
    "                                'answer_start': found_answer['answer_start'],\n",
    "                                'text': found_answer['text'],\n",
    "                            })\n",
    "                        squad_items.append(squad_item)\n",
    "                \n",
    "                group_sentences = ''\n",
    "            else:\n",
    "                group_sentences = tmp_group_sentences\n",
    "            k += 1\n",
    "    return get_best_squad_items(squad_items, top_items=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(mkqa_dataset, lang_code='es', save_path='../artifacts/synthetic/', max_aliases=5):\n",
    "    squad_dataset = {'data': []}\n",
    "    word_tokenizer = get_word_tokenizer()\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # Load config of parsing\n",
    "    config_file = os.path.join(save_path, 'config.json')\n",
    "    if os.path.exists(config_file):\n",
    "        with open(config_file, 'r') as fp:\n",
    "            config = json.load(fp)\n",
    "    else:\n",
    "        with open(config_file, 'w') as fp:\n",
    "            config = {'skipped': [], 'found': []}\n",
    "            json.dump(config, fp)\n",
    "    \n",
    "    # Count items\n",
    "    n_items = sum([1 for _ in mkqa_dataset])\n",
    "    \n",
    "    print('Process items...')\n",
    "    for i, item in enumerate(mkqa_dataset):\n",
    "        idx = 'mkqa_' + str(item['example_id'])\n",
    "        query = item['queries'][lang_code]\n",
    "        \n",
    "        filename = '%s.json' % idx\n",
    "        output_file = os.path.join(save_path, filename)\n",
    "        \n",
    "        # Skip if already parsed\n",
    "        if item['example_id'] in config['found'] or item['example_id'] in config['skipped']:\n",
    "            continue\n",
    "        \n",
    "        print('- Item %d of %d' % (i + 1, n_items), ' '*20, end='\\r')\n",
    "        \n",
    "        parsed_answers = []\n",
    "        answer_types = []\n",
    "        for raw_answer_data in item['answers'][lang_code]:\n",
    "            main_answer = raw_answer_data['text']\n",
    "            aliases = raw_answer_data['aliases'][:max_aliases] if 'aliases' in raw_answer_data else []\n",
    "            iter_parsed_answers = parse_raw_answer_es(raw_answer_data['type'], main_answer, aliases)\n",
    "            for iter_parsed_answer in iter_parsed_answers:\n",
    "                if iter_parsed_answer not in parsed_answers:\n",
    "                    parsed_answers.append(iter_parsed_answer)\n",
    "                    answer_types.append(raw_answer_data['type'])\n",
    "        \n",
    "        if len(parsed_answers) == 0:\n",
    "            # No answers for this query\n",
    "            config['skipped'].append(item['example_id'])\n",
    "            with open(config_file, 'w') as fp:\n",
    "                json.dump(config, fp)\n",
    "            continue\n",
    "        \n",
    "        print('- Item: %d / %d | Found: %d' % (i + 1, n_items, len(config['found'])), ' '*20, end='\\r')\n",
    "        \n",
    "        squad_dataset['data'] = find_squad_item(idx, query, parsed_answers, answer_types, word_tokenizer, lang_code=lang_code,\n",
    "                                                verbose={'i': i+1, 'n_items': n_items, 'n_found': len(config['found'])})\n",
    "        \n",
    "        if len(squad_dataset['data']) == 0:\n",
    "            config['skipped'].append(item['example_id'])\n",
    "            with open(config_file, 'w') as fp:\n",
    "                json.dump(config, fp)\n",
    "            continue\n",
    "        \n",
    "        with open(output_file, 'w') as fp:\n",
    "            json.dump(squad_dataset, fp)\n",
    "        config['found'].append(item['example_id'])\n",
    "        with open(config_file, 'w') as fp:\n",
    "            json.dump(config, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process items...\n",
      "- Item: 2698 / 10000 | Found: 557                                     \n",
      "Unexpected HTTP Status: 400 - http://es.dbpedia.org/resource/Thelma_&_Louise\n",
      "- Item: 4208 / 10000 | Found: 871                                     \n",
      "Unexpected HTTP Status: 400 - http://es.dbpedia.org/resource/Thelma_&_Louise\n",
      "- Item: 4400 / 10000 | Found: 906                                     \n",
      "Unexpected HTTP Status: 404 - http://es.dbpedia.org/resource/Entrenador\n",
      "- Item: 4852 / 10000 | Found: 1000                                     \n",
      "Unexpected HTTP Status: 400 - http://es.dbpedia.org/resource/Law_&_Order\n",
      "- Item: 5259 / 10000 | Found: 1059                                     \n",
      "Unexpected HTTP Status: 400 - http://es.dbpedia.org/resource/Emerson,_Lake_&_Palmer\n",
      "- Item: 6102 / 10000 | Page: 18 / 18 | Found: 1160                     \r"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        main(mkqa_dataset, lang_code='es')\n",
    "        break\n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
